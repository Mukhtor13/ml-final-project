{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8490280,"sourceType":"datasetVersion","datasetId":5003897}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:39:54.713891Z","iopub.execute_input":"2024-05-23T05:39:54.714281Z","iopub.status.idle":"2024-05-23T05:39:54.720241Z","shell.execute_reply.started":"2024-05-23T05:39:54.714253Z","shell.execute_reply":"2024-05-23T05:39:54.719128Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/retail-dataset/train_dataset_with_captions.csv')\ntest_data = pd.read_csv('/kaggle/input/retail-dataset/test_features_with_captions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:39:57.762303Z","iopub.execute_input":"2024-05-23T05:39:57.763051Z","iopub.status.idle":"2024-05-23T05:39:58.386196Z","shell.execute_reply.started":"2024-05-23T05:39:57.763017Z","shell.execute_reply":"2024-05-23T05:39:58.385332Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, titles, descriptions, captions, labels, tokenizer, max_len):\n        self.titles = titles\n        self.descriptions = descriptions\n        self.captions = captions\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.titles)\n\n    def __getitem__(self, idx):\n        title = str(self.titles[idx])\n        description = str(self.descriptions[idx])\n        caption = str(self.captions[idx])\n        text = title + \" \" + caption + \" \" + description\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create dataset objects\nMAX_LEN = 128\ntrain_dataset = CustomDataset(\n    titles=train_data.title.values,\n    descriptions=train_data.description.values,\n    captions=train_data.caption.values,\n    labels=train_data.classes.values,\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\n# DataLoader\nBATCH_SIZE = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:40:00.376609Z","iopub.execute_input":"2024-05-23T05:40:00.377519Z","iopub.status.idle":"2024-05-23T05:40:01.540679Z","shell.execute_reply.started":"2024-05-23T05:40:00.377485Z","shell.execute_reply":"2024-05-23T05:40:01.539876Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee588b1a11d642f6adc93cc08753e01e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abfe0101cd742f9a90a7ecc37683307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52df79dc833d405888fb130abb1178cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95cd8a5f0dc43698f03562a8cab04ba"}},"metadata":{}}]},{"cell_type":"code","source":"class BERTModel(torch.nn.Module):\n    def __init__(self, num_classes):\n        super(BERTModel, self).__init__()\n        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\n# Initialize the model\nNUM_CLASSES = 21\nmodel = BERTModel(NUM_CLASSES)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:40:05.338891Z","iopub.execute_input":"2024-05-23T05:40:05.339593Z","iopub.status.idle":"2024-05-23T05:40:08.502744Z","shell.execute_reply.started":"2024-05-23T05:40:05.339563Z","shell.execute_reply":"2024-05-23T05:40:08.501919Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"626f731662af4fda88a744c5cb381fea"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\ndef train_epoch(model, data_loader, optimizer, device):\n    model = model.train()\n    total_loss = 0\n    for data in data_loader:\n        input_ids = data['input_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        labels = data['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n\n        total_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    return total_loss / len(data_loader)\n\nEPOCHS = 6\nfor epoch in range(EPOCHS):\n    loss = train_epoch(model, train_dataloader, optimizer, device)\n    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {loss}')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T05:40:17.986236Z","iopub.execute_input":"2024-05-23T05:40:17.986993Z","iopub.status.idle":"2024-05-23T06:34:32.022164Z","shell.execute_reply.started":"2024-05-23T05:40:17.986960Z","shell.execute_reply":"2024-05-23T06:34:32.021123Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6, Loss: 0.9471843160832013\nEpoch 2/6, Loss: 0.46772997094086616\nEpoch 3/6, Loss: 0.2862978635998057\nEpoch 4/6, Loss: 0.17797295653324338\nEpoch 5/6, Loss: 0.10787584093805211\nEpoch 6/6, Loss: 0.07238885954270069\n","output_type":"stream"}]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, titles, descriptions, captions, tokenizer, max_len):\n        self.titles = titles\n        self.descriptions = descriptions\n        self.captions = captions\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.titles)\n\n    def __getitem__(self, idx):\n        title = str(self.titles[idx])\n        description = str(self.descriptions[idx])\n        caption = str(self.captions[idx])\n        text = title + \" \" + caption + \" \" + description\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n\n# Create test dataset and dataloader\ntest_dataset = TestDataset(\n    titles=test_data.title.values,\n    descriptions=test_data.description.values,\n    captions=test_data.caption.values,\n    tokenizer=tokenizer,\n    max_len=MAX_LEN\n)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Prediction function\ndef predict(model, data_loader, device):\n    model = model.eval()\n    predictions = []\n    with torch.no_grad():\n        for data in data_loader:\n            input_ids = data['input_ids'].to(device)\n            attention_mask = data['attention_mask'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().numpy())\n    return predictions\n\n# Predict\ntest_predictions = predict(model, test_dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:36:20.278971Z","iopub.execute_input":"2024-05-23T06:36:20.279399Z","iopub.status.idle":"2024-05-23T06:38:22.741298Z","shell.execute_reply.started":"2024-05-23T06:36:20.279366Z","shell.execute_reply":"2024-05-23T06:38:22.740063Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'ID': test_data.ID, 'classes': test_predictions})\nsubmission.to_csv('submission_bert_with_captions_2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:38:34.872954Z","iopub.execute_input":"2024-05-23T06:38:34.873696Z","iopub.status.idle":"2024-05-23T06:38:34.946482Z","shell.execute_reply.started":"2024-05-23T06:38:34.873662Z","shell.execute_reply":"2024-05-23T06:38:34.945705Z"},"trusted":true},"execution_count":8,"outputs":[]}]}